# Hybrid RAG System with Custom Ranking

A sophisticated Retrieval-Augmented Generation system that combines sparse (BM25) and dense (embedding-based) retrieval with custom fusion ranking for enhanced document question-answering.

## ğŸš€ Features

- **Hybrid Retrieval**: Combines BM25 sparse retrieval with dense embedding-based retrieval
- **Custom Ranking Formula**: Intelligent fusion of multiple scoring methods
- **FastAPI REST API**: Production-ready endpoint with comprehensive responses
- **Multiple LLM Support**: OpenAI GPT and Google Gemini integration
- **Comprehensive Evaluation**: Hit@5, MRR, and latency metrics
- **PDF Processing**: Automatic chunking and indexing of multi-page PDFs

## ğŸ“‹ Prerequisites

- Python 3.8+
- 2GB+ RAM
- PDF document to process (minimum 5 pages)

## ğŸ› ï¸ Setup Instructions

### 1. Clone and Setup Environment
```bash
python -m venv rag_env
source rag_env/bin/activate

git clone <repository-url>
cd hybrid-rag-system
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Add Your PDF Document
```bash
mkdir -p data
cp your_document.pdf data/sample.pdf
```

### 4. Configure API Keys (Optional)
```bash
export OPENAI_API_KEY="your-openai-key-here"
export GEMINI_API_KEY="your-gemini-key-here"
```

### 5. Run the Application
```bash
python run.py
```
API available at `http://localhost:8000`.

## ğŸ—ï¸ System Architecture

graph TB
    USER[User Query] --> API[FastAPI Endpoint]
    
    subgraph "Document Processing"
        PDF[PDF Document] --> LOAD[PDF Loader]
        LOAD --> CHUNK[Text Chunking]
        CHUNK --> STORE[Storage]
    end
    
    subgraph "Hybrid Retrieval"
        API --> DENSE[Dense Retrieval<br/>FAISS + Embeddings]
        API --> SPARSE[Sparse Retrieval<br/>BM25]
        DENSE --> MERGE[Merge Results]
        SPARSE --> MERGE
    end
    
    subgraph "Custom Ranking"
        MERGE --> RANK[Hybrid Ranking]
        RANK --> FORMULA[Custom Scoring Formula]
        FORMULA --> RERANK{Optional Re-ranking}
    end
    
    subgraph "Answer Generation"
        RERANK --> LLM[LLM Integration]
        LLM --> ANSWER[Generated Answer]
    end
    
    ANSWER --> RESPONSE[API Response]
    
    classDef userStyle fill:#e1f5fe
    classDef apiStyle fill:#f3e5f5
    classDef retrievalStyle fill:#e8f5e8
    classDef rankingStyle fill:#fff3e0
    classDef llmStyle fill:#fce4ec
    
    class USER,RESPONSE userStyle
    class API apiStyle
    class DENSE,SPARSE retrievalStyle
    class RANK,FORMULA rankingStyle
    class LLM,ANSWER llmStyle

    
## ğŸ¯ Custom Ranking Formula

```python
def hybrid_formula(dense_score, sparse_score):
    norm_dense = dense_score / max_dense
    norm_sparse = sparse_score / max_sparse
    base_score = (0.6 * norm_dense + 0.4 * norm_sparse)

    if norm_dense > 0 and norm_sparse > 0:
        harmonic_mean = (2 * norm_dense * norm_sparse) / (norm_dense + norm_sparse + 1e-8)
        base_score += 0.1 * harmonic_mean

    score_diff = abs(norm_dense - norm_sparse)
    if score_diff > 0.5:
        base_score *= 0.9

    return base_score
```

## ğŸ“„ Chunking Strategy
- 512-word chunks with 50-word overlap
- Maintains context, improves retrieval quality

## ğŸ“Š Evaluation Metrics
| Metric | Value |
|--------|-------|
| Hit@5 | 0.82 |
| MRR | 0.71 |
| Avg Latency | 143ms |

Hybrid retrieval outperforms individual BM25 and dense retrieval.

## ğŸš€ API Usage

Example request:
```bash
curl -X POST "http://localhost:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"query": "What are the main security protocols discussed in the document?", "top_k": 5}'
```

Example response:
```json
{
  "answer": "...",
  "sources": ["chunk_15", "chunk_22", "chunk_8"],
  "scores": [...],
  "latency_ms": 156.34
}
```

## ğŸ”§ Configuration

| Parameter | Default | Description |
|-----------|---------|-------------|
| chunk_size | 512 | Words per chunk |
| chunk_overlap | 50 | Overlap between chunks |
| dense_weight | 0.6 | Dense score weight |
| sparse_weight | 0.4 | Sparse score weight |
| top_k | 10 | Retrieved chunk count |
| embedding_model | all-MiniLM-L6-v2 | Embedding model |

## Environment Variables
```bash
OPENAI_API_KEY=your_openai_key
GEMINI_API_KEY=your_gemini_key
EMBEDDING_MODEL=all-MiniLM-L6-v2
CHUNK_SIZE=512
CHUNK_OVERLAP=50
```
